{
  "hash": "c40b34b469b971af98e3bafbac9d8f1c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Pertemuan 8: Uji Cramer von Mises, Friedman, Quade, Kruskal Wallis, Correlation, Regresi Nonparametrik, Regresi Monotonik, Kernel Smoothing, Regresi Spline\"\nsubtitle: \"Statistika Nonparametrik\"\nimage: static\\regresi_monoton.png\ndescription: \"Offline di Departemen Matematika\"\ndate: 12/03/2024\npage-navigation: true\nformat: html\n---\n\n\n\n# Tabel Guide Uji Nonparametrik\n\n![](static\\tabelconover1.png)\n\n![](static\\tabelconover2.png)\n\n# Uji Cramer von Mises\n\nLindsey, Herzberg dan Watts (1987) memberikan data lebar ruas pertama tarsus kedua untuk dua spesies serangga Chaetocnema. Apakah ini menunjukkan perbedaan populasi antara distribusi lebar untuk kedua spesies?\n\n|Species A|131|134|137|127|128|118|134|129|131|115|\n|---|---|---|---|---|---|---|---|---|---|---|\n|Species B|107|122|144|131|108|118|122|127|125|124|\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- data.frame (Species_A  = c(131,134,137,127,128,118,134,129,131,115),\n                  Species_B = c(107,122,144,131,108,118,122,127,125,124)\n)\ndf\n```\n:::\n\n\n\nTujuan: Akan dilakukan pengujian Cramer Von Mises untuk mengetahui apakah ada perbedaan distribusi antar populasi\n\nHipotesis:\n\n$H_0: F(x) = G(x)$ for all $x$ from $- \\infty$ to  $+ \\infty$\n\n$H_1: F(x) \\neq G(x)$ for at least one value of $x$\n\ndapat digunakan fungsi `cvm_test()` dari library `twosamples`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# install.packages('twosamples')\nlibrary(twosamples)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncvm_test(df$Species_A,df$Species_B)\n```\n:::\n\n\n\nKesimpulannya?\n\n# Uji Friedman\n\nUntuk kelompok tujuh siswa, denyut nadi (per menit) diukur sebelum latihan (I), segera setelah latihan (II), dan 5 menit setelah latihan (III). Data diberikan pada di bawah. Gunakan uji Friedman untuk menguji perbedaan antara denyut nadi pada tiga kesempatan. \n\n![](static\\Friedman_table.png)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nDataset_Friedmann <- data.frame(Student = c('A','A','A','B','B','B','C','C','C','D','D','D','E','E','E','F','F','F','G','G','G'),\n                                Time = rep(c(1,2,3), 7),\n                                Score = c(72, 120, 76, 96, 120, 95, 88, 132, 104, 92, 120, 96, 74, 101, 84, 76, 96, 72, 82,  112, 76))\n```\n:::\n\n\n\nTujuan: Akan dilakukan pengujian Friedman untuk mengetahui apakah terdapat perbedaan denyut nadi per menit pada sebelum latihan, setelah latihan, dan 5 menit setelah latihan untuk kelompok 7 siswa tersebut\n\nHipotesis:\n\n$H_0:$ All treatments have identical effects\n\n$H_1:$ At least one treatment yield larger observed values than at least one other treatment\n\ndapat digunakan fungsi `friedman.test()` dari library `stats`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfriedman.test(y=Dataset_Friedmann$Score, groups=Dataset_Friedmann$Time, blocks=Dataset_Friedmann$Student)\n```\n:::\n\n\n\nKesimpulannya?\n\n# Uji Quade\n\nUji Quade adalah alternatif bagi Uji Friedman. Kedua pengujian memiliki hipotesis null yang sama.\n\nMisal ingin diuji data dari soal pada section Uji Friedmann menggunakan Uji Quade\n\ndapat digunakan fungsi `quade.test()` dari library `stats`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nquade.test(Score ~ Time | Student,\n           data = Dataset_Friedmann)\n```\n:::\n\n\n\nDengan menggunakan fungsi `quadeAllPairsTest()` dari library `PMCMRplus`, kita bisa melakukan *Quade multiple-comparison test*\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# install.packages(PMCMRplus)\nlibrary(PMCMRplus)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nquadeAllPairsTest(y      = Dataset_Friedmann$Score,\n                       groups = Dataset_Friedmann$Time,\n                       blocks = Dataset_Friedmann$Student,\n                       p.adjust.method = \"fdr\")\n```\n:::\n\n\n\nHasil menunjukkan pasangan grup yang berbeda (atau tidak berbeda) secara signifikan\n\n# Uji Kruskal Wallis\n\nLubischew (1962) memberikan pengukuran lebar kepala maksimum dalam satuan 0,01 mm untuk tiga spesies Chaetocnema. Sebagian dari datanya diberikan di bawah ini. Gunakan uji Kruskalâ€“Wallis untuk melihat apakah ada perbedaan lebar kepala untuk ketiga spesies Chaetocnema. \n\n![](static\\Kruskal_wallis_table.png)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- data.frame (Species  = rep(c(\"Species_1\",\"Species_2\",\"Species_3\"),times=c(10,11,8)),\n                  lebar = c(53,50,52,50,49,47,54,51,52,57,49,49,47,54,43,51,49,51,50,46,49,58,51,45,53,49,51,50,51))\n```\n:::\n\n\n\nHipotesis:\n\n$H_0:$ All k population distribution functions are identical\n\n$H_1:$ At least one population yield larger observed values than at least one other population\n\nAtau alternatif $H_1$ lainnya\n\n$H_1:$ The k populations do not all have identical means\n\ndapat digunakan fungsi `kruskal.test()` dari library `stats`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkruskal.test(lebar ~ Species, data = df)\n```\n:::\n\n\n\nKesimpulannya\n\n# Correlation\n\nBardsley dan Chambers (1984) memiliki jumlah sapi ternak (potong) dan domba pada 19 peternakan besar di suatu wilayah. Apakah ada bukti korelasi antara sapi dan domba? \n\n![](static/Correlation_table.png)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- data.frame (Cattle  = c(41,0,42,15,47,0,0,0,56,67,707,368,231,104,132,200,172,146,0),\n                  sheep = c(4716,4605,4951,2745,6592,8934,9165,5917,2618,1105,150,2005,3222,7150,8658,6304,1800,5270,1537))\n```\n:::\n\n\n\n## Spearman's $\\rho$\n\nUji korelasi *Spearman's $\\rho$* dapat dilakukan dengan fungsi `cor.test()` dari library `stats` dengan method `\"spearman\"`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor.test(df$Cattle, df$sheep, method=\"spearman\")\n```\n:::\n\n\n\n## Kendall's $\\tau$\n\nUji korelasi *Kendall's $\\tau$* dapat dilakukan dengan fungsi `cor.test()` dari library `stats` dengan method `\"kendall\"`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor.test(df$Cattle, df$sheep, method=\"kendall\")\n```\n:::\n\n\n\n# Regresi Nonparametrik\n\nA driver kept track of the number of miles she traveled and the number of gallons put in the tank each time she bought gasoline. \n\n| Miles | Gallons |\n|-------|---------|\n| 142   | 11.1    |\n| 116   | 5.7     |\n| 194   | 14.2    |\n| 250   | 15.8    |\n| 88    | 7.5     |\n| 157   | 12.5    |\n| 255   | 17.9    |\n| 159   | 8.8     |\n| 43    | 3.4     |\n| 208   | 15.2    |\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nMiles = c(142, 116, 194, 250, 88, 157, 255, 159, 43, 208)\nGallons = c(11.1, 5.7, 14.2, 15.8, 7.5, 12.5, 17.9, 8.8, 3.4, 15.2)\ndf = data.frame(Gallons, Miles)\ndf\n```\n:::\n\n\n\nLibrary yang akan digunakan:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(stats)\nlibrary(ggplot2)\nlibrary(ggpmisc)\n```\n:::\n\n\n\nA. Draw a diagram showing these points, using gallons as the x-axis \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(Gallons, Miles, xlab = \"Gallons\", ylab = \"Miles\", \n     main = \"Gallons x Miles\", lwd = 2)\n```\n:::\n\n\n\nB. Estimate a and b using the method of least squares \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel1 = lm(Miles ~ Gallons, data=df)\nsummary(model1)\n```\n:::\n\n\n\nC. Plot the least squares regression line on the diagram of part A \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(Gallons, Miles, xlab = \"Gallons\", ylab = \"Miles\", \n     main = \"Gallons x Miles\", lwd = 2)\nabline(reg = model1, col = \"blue\")\n```\n:::\n\n\n\nD. Suppose the EPA estimated this car's mileage at 18 miles per gallon. Test the null hypothesis that this figure applies to this particular car and driver. (Use the test for slope) \n\n$H_0:$ Jarak tempuh mobil ($\\beta$) adalah 18 mil/galon\n\n$H_1:$ Jarak tempuh mobil ($\\beta$) bukan 18 mil/galon\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresults = list()\nbeta0 = 18\n\nfor (i in 1:length(df$Miles)) {\n  results[[i]] <- df$Miles[i] - beta0 * df$Gallons[i]\n}\nresults = unlist(results)\nresults\n\nstat_uji = cor(Gallons, results, method = \"spearman\")\nstat_uji\nabs(stat_uji) > 0.6364\n```\n:::\n\n\n\nE. Find a 95% confidence interval for the mileage of this car and driver\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Function to calculate pairwise slopes\ncalculate_slopes <- function(X, Y) {\n  slopes <- c()\n  n <- length(X)\n  \n  for (i in 1:(n - 1)) {\n    for (j in (i + 1):n) {\n      if (X[j] != X[i]) {  # Avoid division by zero\n        S_ij <- (Y[j] - Y[i]) / (X[j] - X[i])\n        slopes <- c(slopes, S_ij)\n      }\n    }\n  }\n  return(slopes)\n}\n\n# Calculate pairwise slopes\nslopes <- calculate_slopes(df$Gallons, df$Miles)\n\n# Median slope (Theil-Sen estimator)\nmedian_slope <- median(slopes)\n\n# 95% confidence interval\nlower_bound <- quantile(slopes, 0.025)  # 2.5th percentile\nupper_bound <- quantile(slopes, 0.975)  # 97.5th percentile\n\n# Output results\nlist(\n  median_slope = median_slope,\n  lower_bound = lower_bound,\n  upper_bound = upper_bound\n)\n```\n:::\n\n\n\n::: {.callout-tip}\n## Model Alternatif\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Median slope (Theil-Sen estimator)\nmedian_slope <- median(slopes)\n```\n:::\n\n\n\nPada metode kalkulasi confidence interval di atas dapat digunakan untuk membentuk suatu estimasi persamaan regresi nonparametrik dengan median Y dan X sebagai estimator intercept dan $\\beta$ secara berturut-turut.\n:::\n\n# Regresi Monotonik\n\nMetode regresi linier nonparametrik dapat digunakan ketika asumsi regresi linier dapat dipenuhi. Dalam situasi dimana **tidak dapat diasumsikan bahwa garis regresi adalah linier tapi dapat diasumsikan bahwa E(Y|X) naik (minimal tidak turun) dengan meningkatnya X**. Dalam hal ini dinamakan regresinya naik secara monoton.\n\n| Xi  | Yi |\n|-----|----|\n| 0.0 | 30 |\n| 0.5 | 30 |\n| 1.0 | 30 |\n| 1.8 | 28 |\n| 2.2 | 24 |\n| 2.7 | 19 |\n| 4.0 | 17 |\n| 4.0 | 9  |\n| 4.9 | 12 |\n| 5.6 | 12 |\n| 6.0 | 6  |\n| 6.5 | 8  |\n| 7.3 | 4  |\n| 8.0 | 5  |\n| 8.8 | 6  |\n| 9.3 | 4  |\n| 9.8 | 6  |\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Data extracted from the table\ndata <- data.frame(\n  Xi = c(0, 0.5, 1.0, 1.8, 2.2, 2.7, 4.0, 4.0, 4.9, 5.6, 6.0, 6.5, 7.3, 8.0, 8.8, 9.3, 9.8),\n  Yi = c(30, 30, 30, 28, 24, 19, 17, 9, 12, 12, 6, 8, 4, 5, 6, 4, 6)\n)\n\ndata$R_Xi <- rank(data$Xi, ties.method = \"average\")\ndata$R_Yi <- rank(data$Yi, ties.method = \"average\")\n\nn <- nrow(data)\nmean_R_Xi <- mean(data$R_Xi)\nmean_R_Yi <- mean(data$R_Yi)\n\n# Calculate b2 (slope)\nb2 <- sum((data$R_Xi - mean_R_Xi) * (data$R_Yi - mean_R_Yi)) /\n      sum((data$R_Xi - mean_R_Xi)^2)\n\n# Calculate a2 (intercept)\na2 <- mean_R_Yi - b2 * mean_R_Xi\n\n# Estimate R(Y) for Given R(X)\ndata$Rhat_Yi <- a2 + b2 * data$R_Xi\n\n# Convert Rhat_Y back to Y\ninterpolate_Y <- function(R_hat_Y, Yi, R_Yi) {\n  if (R_hat_Y %in% R_Yi) {\n    return(Yi[which(R_Yi == R_hat_Y)])  # Return the Yi if the rank is equal\n  } else if (R_hat_Y < min(R_Yi)) {\n    return(min(Yi))  # Return the largest Yi if the rank is less than the minimum\n  } else if (R_hat_Y > max(R_Yi)) {\n    return(max(Yi))  # Return the largest Yi if the rank is greater than the maximum\n  } else {\n    # Find the nearest ranks for interpolation\n    lower <- max(R_Yi[which(R_Yi < R_hat_Y)])\n    upper <- min(R_Yi[which(R_Yi > R_hat_Y)])\n    \n    Y_lower <- Yi[which(R_Yi == lower)]\n    Y_lower <- Y_lower[1]\n    Y_upper <- Yi[which(R_Yi == upper)]\n    Y_upper <- Y_upper[1]\n    R_lower <- lower\n    R_upper <- upper\n    \n    # Linear interpolation\n    return(Y_lower + (R_hat_Y - R_lower) / (R_upper - R_lower) * (Y_upper - Y_lower))\n  }\n}\n\ndata$Yhat_i <- sapply(data$Rhat_Yi, interpolate_Y, Yi = data$Yi, R_Yi = data$R_Yi)\n\ndata$Rhat_Xi <- (data$R_Yi - a2)/b2\n\ninterpolate_X <- function(R_hat_X, X, R_X) {\n  if (R_hat_X <= min(R_X)) {\n    return(NULL)\n  }\n  if (R_hat_X >= max(R_X)) {\n    return(NULL)\n  }\n  \n  # Find the nearest ranks for interpolation\n  lower <- max(which(R_X <= R_hat_X))\n  upper <- min(which(R_X > R_hat_X))\n  \n  X_lower <- X[lower]\n  X_upper <- X[upper]\n  R_lower <- R_X[lower]\n  R_upper <- R_X[upper]\n  \n  # Linear interpolation\n  return(X_lower + (R_hat_X - R_lower) / (R_upper - R_lower) * (X_upper - X_lower))\n}\n\ndata$Xhat_i <- sapply(data$Rhat_Xi, function(R_hat) interpolate_X(R_hat, data$Xi, data$R_Xi))\ndata\n```\n:::\n\n\n\nNow we can visualize the results\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_clean <- data[!sapply(data$Xhat_i, is.null), ]\n\ncombined <- rbind(\n  as.matrix(data[, c('Xhat_i', 'Yi')]), \n  as.matrix(data[, c('Xi', 'Yhat_i')])\n)\n\ncombined <- as.data.frame(combined)\ncolnames(combined) <- c('X', 'Y')\ncombined <- combined[!sapply(combined$X, is.null), ]\ncombined$X <- unlist(combined$X)\ncombined$Y <- unlist(combined$Y)\ncombined <- combined[order(combined$X), ]\ncombined\n\n# Scatter plot of original data\nplot(data$Xi, data$Yi, pch = 16, col = \"blue\", xlab = \"Xi\", ylab = \"Yi\",\n     main = \"Monotonic Regression\")\n\nlines(combined$X, combined$Y, type = \"b\", col = \"red\", pch = 19, lwd = 2)\n\n# Add a legend\nlegend(\"topright\", legend = c(\"Original Data\", \"Fitted Curve\"), col = c(\"blue\", \"red\"), pch = c(16, NA), lty = c(NA, 1))\n```\n:::\n\n\n\n![](static/regresi_monoton.png)\n\n# Kernel Smoothing\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Contoh 1 ---\n##Library for plots\nlibrary(ggplot2) \nlibrary(ggpubr)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n##Entering data and making data in approprite form to do analysis\nx <- c(11, 22, 33, 44, 50, 56, 67, 70, 78, 89, 90, 100)\ny <- c(2337, 2750, 2301, 2500, 1700, 2100, 1100, 1750, 1000, 1642, 2000, 1932)\ndf <- data.frame(x, y)\n\n## Nadaraya-Watson bandwiths\nplot(df$x, df$y, type = \"l\")\n\nkernsmooth5 <- ksmooth(df$x, df$y, kernel=\"normal\", bandwidth=5)\nlines(kernsmooth5, lwd=2, col='blue')\n\nkernsmooth10 <- ksmooth(df$x, df$y, kernel=\"normal\", bandwidth=10)\nlines(kernsmooth10, lwd=2, col='red')\n\nlegend(\"topleft\",\n       legend = c(\"b = 5\", \"b = 10\"),\n       col = c(\"blue\", \"red\"),\n       lty = 1,\n       cex = 0.6)\n\n##Defining both gaussian and epanechinkov kernel functions\ngaussian_ker <- function(x){\n  val = (1/sqrt(2*pi))*exp(-0.5*x^2)\n  return(val)\n}\n\nepanech_ker <- function(x){\n  ind1  = ifelse((-1 <= x & x<= 1),1,0)\n  val = (3/4)*(1-x^2)*ind1\n  return(val)\n}\n\n##defining bandwidth\nh = 10\n\n##fitting\nfitted_gaussian <- c()\nfitted_epan <- c()\n\n##For loop to find fitted value for both kernel functions\nfor (i in 1:nrow(df)) {\n  x_temp <- df$x[i]\n  \n  temp_denominator_gauss <- 0\n  temp_denominator_epan <- 0\n  temp_num_gauss <- 0\n  temp_num_epan <- 0\n  \n  for (k in 1:nrow(df)) {\n    temp_denominator_gauss = temp_denominator_gauss + gaussian_ker((x_temp - df$x[k])/h)\n    temp_denominator_epan = temp_denominator_epan + epanech_ker((x_temp - df$x[k])/h)\n    temp_num_gauss = temp_num_gauss + gaussian_ker((x_temp - df$x[k])/h)*df$y[k]\n    temp_num_epan = temp_num_epan + epanech_ker((x_temp - df$x[k])/h)*df$y[k]\n  }\n  fitted_gaussian[i] = temp_num_gauss/temp_denominator_gauss\n  fitted_epan[i] = temp_num_epan/temp_denominator_epan\n}\n\nfitted_kern <- ksmooth(df$x, df$y, kernel=\"normal\", bandwidth=h, n.points = nrow(df))\nfitted_kern <- as.data.frame(fitted_kern)$y\n\ndf$fitted_gaussian <- fitted_gaussian\ndf$fitted_epan <- fitted_epan\ndf$fitted_nadaraya <- fitted_kern\n\n##Data frame for all fitted values is given as:\ndf\n\n##For plotting\np1 <- ggplot(df, aes(x = x, y = fitted_gaussian)) + geom_point() +\n  geom_point(aes(y = y), col = \"red\")+ geom_line() \n\np2 <- ggplot(df, aes(x = x, y = fitted_epan)) + geom_point() +\n  geom_point(aes(y = y), col = \"red\") + geom_line() \n\np3 <- ggplot(df, aes(x = x, y = fitted_nadaraya)) + geom_point() +\n  geom_point(aes(y = y), col = \"red\") + geom_line() \n\nggarrange(p1, p2, p3, labels  = c(\"Gaussian\", \"Epanechnikov\", \"Nadaraya-Watson\"), nrow = 1, ncol = 3)\n```\n:::\n\n\n\n# Regresi Spline\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(splines)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#Spline regression\nDay = c(5,10,25,45,70,85,90,100,110,125,130,150,175)\nSales = c(100,125,250,300,350,460,510,460,430,400,370,340,330)\ndata = data.frame(Day,Sales)\n\nggplot(data, aes(x=Day, y = Sales)) + geom_point()\n\nmodel_linear = lm(Sales ~Day, data = data)\nsummary(model_linear)\nplot(Sales~Day)\nlines(data$Day, predict(model_linear), col=\"red\")\ndata\n\n#Model Spline\nmodel_spline = lm(Sales ~ bs(Day, knots = c(10, 25, 70, 90,100,150)))\nsummary(model_spline)\n\nplot(Sales ~ Day)\nlines(data$Day, predict(model_spline), col = \"red\")\n\nquantile(Day, 0.2) #20%\nquantile(Day, 0.4) #40%\nquantile(Day, 0.6) #60%\nquantile(Day, 0.8) #80%\n\nmodel_quantile = lm(Sales ~ bs(Day, knots = c(33,82,102,128)))\nsummary(model_quantile)\nplot(model_quantile)\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}